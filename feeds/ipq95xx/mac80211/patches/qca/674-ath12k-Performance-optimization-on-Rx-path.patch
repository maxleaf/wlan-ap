From 21076c688f82c7fb5d36d1b92655450455d72fca Mon Sep 17 00:00:00 2001
From: Balamurugan Selvarajan <quic_bselvara@quicinc.com>
Date: Tue, 4 Oct 2022 12:03:45 +0530
Subject: [PATCH] ath12k: Performance optimization in Rx path

optimized dp_service_srng(). There was multiple de reference to check
various mask. This has been optimized to read in one shot to avoid cache miss.

Added branch predictions in Rx processing.

Changed napi_gro_recieve() to netif_receive_skb() in fast_rx path.

Introduced cached entry to get the next descriptor in rx path.

With all the above changes, The CPU idle percentage is increased to 3 to 4%
and see a increase in TP of 100 to 150 Mbps in UDP uplink.

Signed-off-by: Balamurugan Selvarajan <quic_bselvara@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/dp.c    | 36 +++++++++++++++----------
 drivers/net/wireless/ath/ath12k/dp_rx.c | 33 ++++++++++++-----------
 drivers/net/wireless/ath/ath12k/pci.c   | 11 +++++++-
 3 files changed, 50 insertions(+), 30 deletions(-)

Index: backports-20220404-5.4.164-f40abb4788/drivers/net/wireless/ath/ath12k/dp.c
===================================================================
--- backports-20220404-5.4.164-f40abb4788.orig/drivers/net/wireless/ath/ath12k/dp.c
+++ backports-20220404-5.4.164-f40abb4788/drivers/net/wireless/ath/ath12k/dp.c
@@ -911,22 +911,37 @@ int ath12k_dp_service_srng(struct ath12k
 	int i = 0, j;
 	int tot_work_done = 0;
 	bool flag;
+	struct ath12k_hw_ring_mask *ring_mask = ab->hw_params.ring_mask;
+	u8 tx_mask = ring_mask->tx[grp_id];
+	u8 rx_err_mask = ring_mask->rx_err[grp_id];
+	u8 rx_wbm_rel_mask = ring_mask->rx_wbm_rel[grp_id];
+	u8 rx_mask = ring_mask->rx[grp_id];
+	u8 reo_status_mask = ring_mask->reo_status[grp_id];
+	u8 host2rxdma_mask = ring_mask->host2rxdma[grp_id];
+	u8 rx_mon_dest_mask = ring_mask->rx_mon_dest[grp_id];
+	u8 tx_mon_dest_mask = ring_mask->tx_mon_dest[grp_id];
+	u8 misc_intr_mask = rx_err_mask |
+			    rx_mon_dest_mask |
+			    tx_mon_dest_mask |
+			    reo_status_mask |
+			    host2rxdma_mask;
 
-
-        if (ab->hw_params.ring_mask->tx[grp_id]) {
-                i = __fls(ab->hw_params.ring_mask->tx[grp_id]);
+        if (tx_mask) {
+                i = __fls(tx_mask);
                 ath12k_dp_tx_completion_handler(ab, (i == 4) ? 3 : i);
         }
 
-	if (ab->hw_params.ring_mask->rx_err[grp_id]) {
-		work_done = ath12k_dp_process_rx_err(ab, napi, budget);
+	if (rx_mask) {
+		i =  fls(rx_mask) - 1;
+		work_done = ath12k_dp_process_rx(ab, i, napi,
+						 budget);
 		budget -= work_done;
 		tot_work_done += work_done;
 		if (budget <= 0)
 			goto done;
 	}
 
-	if (ab->hw_params.ring_mask->rx_wbm_rel[grp_id]) {
+	if (rx_wbm_rel_mask) {
 		work_done = ath12k_dp_rx_process_wbm_err(ab,
 							 napi,
 							 budget);
@@ -937,24 +952,25 @@ int ath12k_dp_service_srng(struct ath12k
 			goto done;
 	}
 
-	if (ab->hw_params.ring_mask->rx[grp_id]) {
-		i =  fls(ab->hw_params.ring_mask->rx[grp_id]) - 1;
-		work_done = ath12k_dp_process_rx(ab, i, napi,
-						 budget);
+	if (!misc_intr_mask)
+		goto done;
+
+	if (rx_err_mask) {
+		work_done = ath12k_dp_process_rx_err(ab, napi, budget);
 		budget -= work_done;
 		tot_work_done += work_done;
 		if (budget <= 0)
 			goto done;
 	}
 
-	if (ab->hw_params.ring_mask->rx_mon_dest[grp_id]) {
+	if (rx_mon_dest_mask) {
 		for (i = 0; i < ab->num_radios; i++) {
 			for (j = 0; j < ab->hw_params.num_rxmda_per_pdev; j++) {
 				int id = i * ab->hw_params.num_rxmda_per_pdev + j;
 
 				flag = ATH12K_DP_RX_MONITOR_MODE;
 
-				if (ab->hw_params.ring_mask->rx_mon_dest[grp_id] &
+				if (rx_mon_dest_mask &
 					BIT(id)) {
 					work_done =
 					ath12k_dp_mon_process_ring(ab, id, napi, budget,
@@ -969,14 +985,14 @@ int ath12k_dp_service_srng(struct ath12k
 		}
 	}
 
-	if (ab->hw_params.ring_mask->tx_mon_dest[grp_id]) {
+	if (tx_mon_dest_mask) {
 		for (i = 0; i < ab->num_radios; i++) {
 			for (j = 0; j < ab->hw_params.num_rxmda_per_pdev; j++) {
 				int id = i * ab->hw_params.num_rxmda_per_pdev + j;
 
 				flag = ATH12K_DP_TX_MONITOR_MODE;
 
-				if (ab->hw_params.ring_mask->tx_mon_dest[grp_id] &
+				if (tx_mon_dest_mask &
 					BIT(id)) {
 					work_done =
 					ath12k_dp_mon_process_ring(ab, id, napi, budget,
@@ -991,10 +1007,10 @@ int ath12k_dp_service_srng(struct ath12k
 		}
 	}
 
-	if (ab->hw_params.ring_mask->reo_status[grp_id])
+	if (reo_status_mask)
 		ath12k_dp_process_reo_status(ab);
 
-	if (ab->hw_params.ring_mask->host2rxdma[grp_id]) {
+	if (host2rxdma_mask) {
 		struct ath12k_dp *dp = &ab->dp;
 		struct dp_rxdma_ring *rx_ring = &dp->rx_refill_buf_ring;
 
Index: backports-20220404-5.4.164-f40abb4788/drivers/net/wireless/ath/ath12k/dp_rx.c
===================================================================
--- backports-20220404-5.4.164-f40abb4788.orig/drivers/net/wireless/ath/ath12k/dp_rx.c
+++ backports-20220404-5.4.164-f40abb4788/drivers/net/wireless/ath/ath12k/dp_rx.c
@@ -342,7 +342,7 @@ int ath12k_dp_rxbufs_replenish(struct at
 	while (num_remain > 0) {
 		skb = dev_alloc_skb(DP_RX_BUFFER_SIZE +
 				    DP_RX_BUFFER_ALIGN_SIZE);
-		if (!skb)
+		if (unlikely(!skb))
 			break;
 
 		if (!IS_ALIGNED((unsigned long)skb->data,
@@ -362,7 +362,7 @@ int ath12k_dp_rxbufs_replenish(struct at
 		 * and check idr is working, later confirm hw cc is working by
 		 * checking magic. remove all TODO's after testing
 		 */
-		if (hw_cc) {
+		if (likely(hw_cc)) {
 			spin_lock_bh(&dp->rx_desc_lock);
 
 			/* Get desc from free list and store in used list
@@ -2534,7 +2534,6 @@ static void ath12k_dp_rx_h_mpdu(struct a
 	u32 vp;
 #endif
 	struct wireless_dev *wdev = NULL;
-	struct ath12k_sta *arsta = NULL;
 
 	/* PN for multicast packets will be checked in mac80211 */
 	rxcb = ATH12K_SKB_RXCB(msdu);
@@ -2568,11 +2567,7 @@ static void ath12k_dp_rx_h_mpdu(struct a
 				}
 #endif
 				msdu->protocol = eth_type_trans(msdu, msdu->dev);
-				napi_gro_receive(rxcb->napi, msdu);
-				if (peer->sta)
-					arsta =
-				        (struct ath12k_sta *)peer->sta->drv_priv;
-
+				netif_receive_skb(msdu);
 				return;
 		        }
 		}
@@ -3023,6 +3018,7 @@ int ath12k_dp_process_rx(struct ath12k_b
 	struct ath12k_sta *arsta = NULL;
 	struct ath12k_peer *peer = NULL;
 	struct ath12k *ar;
+	int valid_entries;
 
 	__skb_queue_head_init(&msdu_list);
 
@@ -3032,8 +3028,15 @@ int ath12k_dp_process_rx(struct ath12k_b
 
 try_again:
 	ath12k_hal_srng_access_begin(ab, srng);
+	valid_entries = ath12k_hal_srng_dst_num_free(ab, srng, false);
+	if (unlikely(!valid_entries)) {
+		ath12k_hal_srng_access_end(ab, srng);
+		spin_unlock_bh(&srng->lock);
+		return -EINVAL;
+	}
+	ath12k_hal_srng_dst_invalidate_entry(ab, srng, valid_entries);
 
-	while ((rx_desc = ath12k_hal_srng_dst_get_next_entry(ab, srng))) {
+	while (likely((rx_desc = ath12k_hal_srng_dst_get_next_cache_entry(ab, srng)))) {
 		struct hal_reo_dest_ring desc = *(struct hal_reo_dest_ring *)rx_desc;
 		enum hal_reo_dest_ring_push_reason push_reason;
 		u32 cookie;
@@ -3050,10 +3053,10 @@ try_again:
 				desc.buf_va_lo);
 
 		/* retry manual desc retrieval */
-		if (!desc_info)
+		if (unlikely(!desc_info))
 			desc_info = ath12k_dp_get_rx_desc(ab, cookie);
 
-		if (desc_info->magic != ATH12K_DP_RX_DESC_MAGIC)
+		if (unlikely(desc_info->magic != ATH12K_DP_RX_DESC_MAGIC))
 			ath12k_warn(ab, "Check HW CC implementation");
 
 		msdu = desc_info->skb;
@@ -3072,8 +3075,8 @@ try_again:
 
 		push_reason = FIELD_GET(HAL_REO_DEST_RING_INFO0_PUSH_REASON,
 					desc.info0);
-		if (push_reason !=
-		    HAL_REO_DEST_RING_PUSH_REASON_ROUTING_INSTRUCTION) {
+		if (unlikely(push_reason !=
+		    HAL_REO_DEST_RING_PUSH_REASON_ROUTING_INSTRUCTION)) {
 			dev_kfree_skb_any(msdu);
 			ab->soc_stats.hal_reo_error[dp->reo_dst_ring[ring_id].ring_id]++;
 			continue;
@@ -3090,7 +3093,7 @@ try_again:
 					  desc.rx_mpdu_info.peer_meta_data);
 		rxcb->tid = FIELD_GET(RX_MPDU_DESC_INFO0_TID, desc.rx_mpdu_info.info0);
 
-		if (ath12k_debugfs_is_extd_rx_stats_enabled(ar) && rxcb->peer_id) {
+		if (unlikely(ath12k_debugfs_is_extd_rx_stats_enabled(ar) && rxcb->peer_id)) {
 			rcu_read_lock();
 			spin_lock_bh(&ab->base_lock);
 			peer = ath12k_peer_find_by_id(ab, rxcb->peer_id);
@@ -3131,7 +3134,7 @@ try_again:
 
 	spin_unlock_bh(&srng->lock);
 
-	if (!total_msdu_reaped)
+	if (unlikely(!total_msdu_reaped))
 		goto exit;
 
 	//TODO Move to implicit BM?
